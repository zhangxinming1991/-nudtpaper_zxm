\chapter{绪论}

随着互联网的不断发展，以及终端设备，比如手机，拍照设备，监控设备的广泛应用，互联网上现在保存的图片数量巨大，并且每天都在以惊人的速度增长，有数据显示，2011年FaceBook上就存储了多达1000亿张图片\upcite{facebook}。如何在巨大的图像数据集中精确并且快速的找到相应的图片，逐渐发展为一个十分热门的研究课题。该问题是大数据处理及图像处理两个计算机领域问题的交叉。大数据处理技术在近年来无疑是最受关注的研究问题之一，其他的还有云计算及人工智能，三者有着紧密的联系。基于内容的图像检索技术(content-based Image Retrivel或者CBIR)\upcite{CBIR}在大数据下的图像检索（Image Retrivel）\upcite{imagerv}背景下受到研究者们的重视，该技术充分利用图片自身的信息，不需要人工的为每张图片打上描述标签，较传统的以标签搜图技术，有极大的匹配精度。现在百度\upcite{baiduPic} 和google 公司都有相应的以图搜图服务，用户通过上传相似图片，搜索系统将返回匹配成功的相似结果。CBIR技术涵盖了许多图像处理中的重要环节，比如特征提取，特征匹配等，本文的研究工作是围绕特征提取进行的。

\section{研究背景}
本小节先介绍图像检索的基本概念，然后给出两种图像检索技术的实例，分别是基于文本的图像检索（TBIR）和基于图像内容检索（CBIR）。本文对两种技术进行比较，分析各自的优点和缺点，展现CBIR的存在意义及价值，最后对海量图像内容检索中的关键步骤展开分析。

\subsection{图像检索}
在1970时就出现了图像检索技术\upcite{imagerv}，当时主要还是通过输入检索文字的方式来进行图片的搜索，比如Getty AAT\upcite{GettyAAT}，使用了13300个词汇来描述其下的所有图片，包括历史的，艺术的，建筑的等不同领域的图片。又如 Gograph，将其下的图片分为了30多级的检索主目录，每级目录下有含有子目录，层层递归，来划分图片的不同归类，以便于人们进行查找。上面两个例子就是以典型的文本的方式来进行图片检索的例子。

得益于研究者们的大量研究贡献，以文本的图片检索技术发展十分迅猛，现在已经有相当多的这种方式下的检索算法，比如RageRank\upcite{PageRank}算法，聚类\upcite{TextClustering}算法，语言解析算法等等，但是因为受到描述词汇的限制，描述词汇的模糊和多义性，在大数据背景下的图片检索，文本方式的图片检索效果并不理想。为了更加准确的表达搜索的意图，更加精确的搜索目标图片，基于内容的图像检索（Content-based Image Retrieval）\upcite{CBIR}技术逐渐被研究者们重视起来。

CBIR技术和TBIR技术是两种不一样的检索技术，基于内容的图像检索技术利用图片自身的信息，这里的信息可以是图片的颜色，图片的形状或者图片的纹理，同时特征可以是全局的，也可以是局部的，该技术利用上述的这些特征，然后是相应的匹配算法去查找目标图像。因为查找的信息十分精确，没有描述模糊，描述歧义等限制，因此如果提取的特征的精度和匹配算法精度同时很高，那么CBIR技术的查询返回的结果非常准确。但是由于图片的特征信息量大，因此查找的速度也会受到其制约。目前，已经存在有基于内容图像检索的商用系统，比如QBIC系统\upcite{QBIC}，WebSeek系统\upcite{webseek}，PhotoBook系统\upcite{Photobook}，百度公司研发的百度图片系统及谷歌公司研发的google picture等。CBIR系统的运行原理架构如图\ref{fig:CBIR_fw}所示:
\begin{figure}[htp]
\centering
\includegraphics{retrivelfw}
\caption{基于内容的图像检索框架}
\label{fig:CBIR_fw}
\end{figure}
\\先将检索系统的图像库通过特征提取模块进行特征提取，将提取出来的特征保存到系统的特征库中，查询图像先通过特征提取模块进行特征提取，然后检索模块通过匹配查询图片的特征和特征库存在的特征，最后返回匹配的结果。

上面运行步骤中，主要涉及两个图像处理的中关键技术，分别是特征提取和特征匹配。特征提取最为重要，因为如果特征提取做的不好，会直接影响后面的所有步骤，因此如果快速并且提取高质量的图像特征，一直是研究的热点。

\subsection{特征提取}
因为在计算机中，处理的只有数字信号，为了使计算机能够”理解“图像，我们必须从图像中提取有用的信息，得到图像的”非图像“表示或描述，比如数值，向量或者是符号，这一过程就是图像的特征提取。
\subsubsection{特征分类}
图像特征目前没有特别严格的定义，大体可以分为两类，分别是全局特征和局部特征。常见的全局特征有图片颜色信息，纹理信息，轮廓形状信息，全局特征可以较好的理解图片的总体信息及内容，但是却不太适合进行图片的匹配工作，比如一张图片为红色，如果使用红色这一特征进行匹配，那么很可能搜到许多符合条件的图片。相反局部特征就很好的胜任图片匹配这一工作。常见的局部特征有，图像的斑点信息，图像的角点信息，图片的边缘信息及图片中的脊，局部特征是一张图片中具有局部不变性的，具有图片强烈代表性的局部信息，这些信息即使在图片受到光照，遮挡，旋转等外界影响的情况，也能很好的表示图像的独特性。因此局部特征较好的应用于图像的匹配工作，而不太适合从整体上理解图像，本文研究的SIFT算法就是一种局部特征的提取算法。以下是一些常见的全局及局部特征的定义解释：
\begin{itemize}
\item 颜色特征\\颜色特征\upcite{HSVColor1, HSVColor2}是一种全局特征,描述图像表面的特性。由于颜色特征没有很好的捕捉物体的局部特性，因此当在一个很大的数据库中，使用颜色特征进行图像匹配时，会检出很多满足要求的结果。颜色特征的优点在于其不受物体大小尺寸变化及物体空间位移变化的影响。
\item 纹理特征\\纹理特征\upcite{TextureFeature, TextureFeature2}也是一种全局特征，和颜色特征相同的是，纹理特征也是一种图像物体表面的特性。和颜色特征不一样的是，纹理特征不是基于单个像素点进行纹理分析的，它是在一个像素区域上进行统计意义上的纹理分析，这样提取的纹理特征具有旋转不变性，抗噪声能力较强。纹理特征的缺点在于很容易受到分辨率及光照的影响，同时3D物体投射成2D时的纹理特征可能会发生变化。
\item 形状特征\\各种基于形状特征\upcite{ShapeFeature}的检索方法可以任意选取图像中的区域形状进行检索，简单快捷。但是形状特征有几个缺陷，第一的形状特征提取方法现在还没有完整的数学模型。第二当物体发生了一定形变时，基于形状特征进行匹配时就会失效，但是现实生活中物体的外部形状很可能发生变化，最简单的就是热胀冷缩。第三，3D 物体进行平面投射成2D 时，由于视觉的变化，物体的轮廓形状是发生变化的，提取出来的2D 的形状特征并不能反映出原来3D物体的形状特性，在一定程度上已经发生了失真。
\item 空间关系特征\\空间关系特征是一种全局特征，它描述了图片中物体和物体间的空间关系，空间关系可以是邻接，可以是重叠，也可以是包含等，通过空间特征关系，我们就可以对图片中物体的整体位置关系有一个主体的框架。空间关系可以被划分为两种，分别是绝对的空间关系和相对空间关系。绝对空间关系描述的是物体间的绝对位置关系，通过绝对空间关系特征，我们可以描述出目标和目标间的距离大小以及方位。而相对空间关系特征描述的是目标与目标间的相对位置关系，比如是上下关系，左右关系等。通过绝对的空间位置，可以感知出目标的相对位置。空间关系特征的优点在于可以很直观的理解整幅图片的物体的空间关系，但是空间关系特征相对简单，信息量小，对物体旋转，偏移，尺度变化等噪声抵抗力不好。很多时候，空间关系特征需要和其他的一些特征配合着进行使用。
\item 角点 \\角点\upcite{CornPoint,CornPoint2}经常是被检测在边缘的交界处、被遮挡的边缘、纹理性很强的部分。通过这些角点，可以很好的表示出图像的独特性。
\item 斑点\\斑点通常是指与周围有着颜色和灰度差别的区域，如草原上的一棵树或一栋房子，一个湖泊中的一只小船。由于斑点代表的是一个区域，相比单纯的角点，它的稳定性要好，抗噪声能力要强，所以它在图像配准上扮演了很重要的角色。本文研究的SIFT算法也是就是一种斑点检测方法。
\item 边缘 \\边缘是组成两个图像区域之间边界（或边缘）的像素。一般一个边缘的形状可以是任意的，还可能包括交叉点。在实践中边缘一般被定义为图像中拥有大的梯度的点组成的子集。一些常用的算法还会把梯度高的点联系起来来构成一个更完善的边缘的描写。这些算法也可能对边缘提出一些限制。
\item 脊 \\长条形的物体被称为脊。在实践中脊可以被看作是代表对称轴的一维曲线，此外局部针对于每个脊像素有一个脊宽度。从灰梯度图像中提取脊要比提取边缘、角和区域困难。在空中摄影中往往使用脊检测来分辨道路，在医学图像中它被用来分辨血管。
\end{itemize}
\subsubsection{提取算法}
不同类型的特征对应着不同的提取算法，算法的原理不一样，提取精度不一样，计算复杂性也不一样。
\begin{itemize}
\item 颜色直方图法\upcite{ColorHistogram}\\颜色是通过将图片中的全局区域的R,G,B颜色剥离，然后统计各个颜色出现的概率，得到颜色分布式的直方图的一种特征提取方法。因为颜色直方图法是一种全局概率统计的方法，因此它和物体的空间关系是没有太大关系，当物体发生了旋转，尺度缩放等物理空间变化是，不是会影响颜色直方图法提取的结果的。但是正是由于它对物理位置不敏感，会导致许多图片中物体关系是完全不一致的，但是颜色直方图是一样的，利用该直方图进行匹配时，就会出现许多误判。
\item 灰度梯度共生矩阵\upcite{GLCM}\\ 灰度梯度共生矩阵（Gray-GradientCo-occurrence Matrix）方法是一种具有统计意义的纹理特征分析方法。该方法统计图像的梯度及灰度信息，然后使用一个矩阵保存提取到的信息。GGCM的描述方法的优势在于可以很好的描述图像灰度和梯度的变化规律，同时也很好的描述了图像的纹理特征，特别适用于具有方向性的纹理描述。在灰度梯度共生矩阵的基础上，可以进行很多其他图像描述维度信息的二次统计，比如灰度平均、梯度平均、灰度均方差、梯度均方差等等，有相关的研究者\upcite{GLCM2}在研究灰度梯度共生矩阵的基础上，提出了四个维度的信息，分别是能量、惯量、熵和相关性。
\item 几何法\\几何法其实也是一种针对图像纹理信息的特征提取方法。根据纹理理论，纹理信息很多时候是具有规则的几何特征的，因此可以使用几何的方法来进行图像纹理信息的提取。在众多的几何提取算法中，有两种方法最有影响力，分别是Voronio 棋盘格特征法\upcite{VoronoiDiagrams}和结构法。
\item 边界特征法\\边界特征法是一种针对图像轮廓形状的特征提取方法。在边界特征提取算法中，Hough 变换检测平行直线方法\upcite{HoughTransform}和边界方向直方图方法\upcite{Boundary}是两种十分经典提取算法。Hough 变换检测平行直线方法是将图片中物体边界处具有相同特征的点归类聚集在一起，最终将物体的外围形状轮廓提取出来。Hough方法在物体空间参数不是很大的情况下，提取的总体效果还是很不错的，但是当物体空间很大时，该算法的时间复杂性和空间复杂度会急剧上升。边界方向直方图法是先利用微分求出物体的边缘方向，然后做出方向的直方图，最后提取出物体的轮廓形状特征信息。
\item 傅里叶形状描述符法\\傅里叶形状描述符(Fourier shape deors)\upcite{Fourier}方法是对图像的轮廓进行傅里叶变换，通过傅里叶变化，物体的边界的连续性，边界性及封闭性均能很好的表现出来。同时傅里叶形状描述法还有降维的优点，可以将二维问题转换为一维问题。
\item 尺度不变特征转换法\\尺度不变特征转换(Scale-invariant feature transform或SIFT)\upcite{SIFT}是一种局部特征提取算法，该算法先对目标图像构建尺度空间，然后在尺度空间中寻找一些具有尺度不变性的特征点，将这些点提取出来，使用一维向量的形式对特征点进行描述，最终形成图片的特征点。SIFT算法提出来的特征点抗噪声能力十分强大，在物体被光照，遮挡，旋转等影响因素干扰下，算法依然可以十分高的精度将物体识别出来，是一个具有划时代意义的算法。
\item Harris角点检测法\upcite{Harris}\\该算法的基本思想是，使用一个固定大小的检测窗口进行平移滑动检测，如果窗口在任意方向上的滑动变化，都会是使得图像的灰度发生明显的变化，那么就可以认为这窗口区域存在角点。检测的窗口一般是高斯函数，即使用高斯函数和图片的固定区域不断进行卷积操作。Harris角点检测算法检出的角点对噪声有非常好的鲁棒性，稳定性好，计算效率高。
\end{itemize}

在众多的提取算法中，综合考虑了算法提取的精度，算法的时间复杂度及空间复杂度以及可以突显大数据处理框架的在大规模处理时的高效性，本文选取了SIFT算法\upcite{SIFT2}作为研究点，将其应用于大数据处理框架Spark上，完成大规模图像库的快速提取工作，本文将在第二章详细介绍SIFT 算法的原理以Spark的基本原理。

\subsection{特征匹配}
除了特征提取之外，特征匹配也是图像处理中十分重要的技术。对于基于内容的图像检索，其匹配实际就是相似搜索\upcite{SimilaritySearche}或者是近似搜索\upcite{ProximitySearch}问题。相似搜索是指对于给定的查询数据对象，在目标对象集合中找到和查询对象相似的对象。匹配的关键在于相似度的定义，常见的相似度的度量方式有下面几种：
\begin{itemize}
\item 汉明距离。对于任意两个字符串x和y，它们的汉明距离计算如公式\ref{hanming}所示：
\begin{equation}\label{hanming}
Ha=\sum_{1}^d(x_i=y_j)
\end{equation}

\item $l_p$ 范数距离。对于d维实数空间$R_d$中的两个向量点x=($x_1$,...,$x_d$)和y=($y_1$,...,$y_d$)，x和y之间$l_p$-范数距离定义为:
\begin{equation}\label{fanshu}
L_p(x_i,y_i)=\Bigl(\sum_{l=1}^n(x_i-y_i)^p\Bigr)^\frac{1}{p}
\end{equation}

\item 余弦距离。设x和y为对于d维实数空间$R_d$中两个非零向量，它们的余弦距离计算公式如下：
\begin{equation}\label{yuxian}
\cos(x,y)=\frac{xy}{|x|_2|y|_2}
\end{equation}

\item Jaccard距离。对于两个集合A和B，它们之间的Jaccard距离的计算公式如下：
\begin{equation}\label{Jaccard}
Ja(A,B)=\frac{|{A}\cap{B}|}{|{A}\cup{B}|}
\end{equation}
\end{itemize}

特征向量的维度越高，特征的描述也越详细，匹配的精度也就越高，如常见的sift特征是128维，Gist特征是960维，深度学习提取的特征维度是1024维甚至更高。在面对着这些高维的特征向量时，如果完全单纯的计算每个向量间的距离，匹配速度将会十分缓慢，因此如何在保证精度的情况下，提升匹配的效率，是一个技术难点。面对高维特征向量，可以通过给特征建立有效的索引来提升查询效率，比如KD-tree\upcite{KDTree}，R-Tree\upcite{RTree}或者SR-tree\upcite{SRtree}等，这些方法的基本思想是将有相同特性或者是相近的特性的特征点统一划分到一个区域上，然后给这个区域建立一个索引，查找时根据索引先确定某个区域，然后再在区域中进一步查找，从而提升查找的效率。建立索引方式在面对着超高维特征向量时，效率还是偏低，有学者提出了降维的思想，将超高维向量降低到某个维度，牺牲部分精度来大幅度提升查询效率，这种思想的最著名实现就是局部敏感哈希算法\upcite{LSH}，该方法通过适当的调节参数，可以保证很高的精确度条件下也有很高的查询效率。该算法目前已经被广泛的应用于文本聚类，语言统计等领域。

特征匹配方法在本设计工作中只是充当验证特征提取正确性的角色，并不是研究的重点，因此在第二章的基本原理部分没有对特征匹配进行深入的原理分析。

\subsection{大数据处理处理技术}
我们现在生活在一个信息量爆炸的时代，各行各业现在都面临着数据处理的压力，无论是处理的数据量，还是处理的速度，单台机器早已无法满足当今大数据应用的处理要求了，因此我们需要有一个大数据背景下的数据处理或者是存储框架，以帮助我们更加轻松的处理海量数据。在这种背景下，许多优秀的大数据处理框架逐渐诞生。接下来，本文将会介绍大数据内存计算引擎Spark和分布式文件系统HDFS两个大数据处理框架，它们都成功的应用于本文的设计工作中。
\subsubsection{内存计算框架spark}
Apache Spark 是专门为大规模数据处理而设计的快速通用的计算引擎，它的开发者为UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)。Spark的最大特点在于其内存持久化技术，可以将计算过程中的中间结果保存到内存，大大加快了计算任务的速度，十分适合迭代次数多的作业。Spark本质上也是一个Mapreduce框架，这点是和Hadoop相同的，将计算任务Map Task分发到各个节点上，在计算结束后，通过Reduce Task统一收集或者保存到HDFS中，区别还是在于Spark的中间结果支持内存缓存机制。Spark提供了一套完成的内存编程模型及容错重算机制，使得开发者可以像编写普通程序一样去编写内存作业的应用程序，也不用担心缓存在内存的中间结果因为某些原因而丢失。

除了Map和Reduce操作之外，Spark还提供了丰富的生态应用环境，比如提供了SQL与结构化数据处理工具Spark SQL，机器学习工具MLib，分布式图计算引擎Graphx以及流式处理工具Spark Streaming。同时Spark的编程接口十分丰富，支持Scala，java,Python和R语言。Spark框架如图~\ref{fig:sparkfw}所示：
\begin{figure}[htp]
\centering
\includegraphics{sparkfw}
\caption{spark框架}
\label{fig:sparkfw}
\end{figure}

Spark的核心是RDD(Resilient Distributed Datasets)\upcite{Spark}，RDD是一个分布式内存数据结构，可以将内存存储透明化，让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。RDD的编程接口非常丰富，基本可以满足所有常见开发的需求。RDD的接口操作也成为算子，比如Map算子，FlatMap算子，filter算子，这些算子都是monad模式的算子，即是不带key特性的单边数据处理。同时Spark的RDD接口中也提供了丰富的Key特性处理接口，比如join，groupByKey，reduceByKey等算子。通过Spark下这些内存运行运算算子，我们可以很容易的实现单机到分布式的功能转换。

在本文的设计工作中，我们将Spark作为大规模图像特征提取的计算引擎，加速特征提取的工作。
\subsubsection{分布式文件系统HDFS}
大数据背景下的需要处理的数据往往是非常大的，很多时候都是TB级别的，传统的单机的数据存储方式是无法满足读写速度要求及数据存储可靠性，这这种情况下，需要一种数据的分布式存储方案，将处理的数据分散存储而有集中管理，分布式文件系统HDFS就是这样的一种存储方案。虽然现在已有的网络文件系统（NFS）也算是一种分布式文件系统，但是和HDFS的存储方案及执行效率，安全性相比，HDFS绝对是完胜传统的NFS架构。因为NFS本质上还是单机的存储方式，只不过访问端和存储端是处于分布式的状态，一旦客户端的访问量过大，就会造成服务端拥堵。而次，NFS的安全可靠性也不如HDFS，在HDFS的有完整的大数据场景下的文件备份方案，保证某个节点数据损坏后，可以迅速恢复。在数据的同步性上，HDFS的效率会高于NFS，因为在NFS下，客户端的写操作必须要先上传服务端，其他的客户端才能感知到数据的修改，在HDFS下则不需要先经过服务器端这一操作。

HDFS，是Hadoop Distributed File System的简称，是Hadoop抽象文件系统的一种实现。HDFS是Google大数据三大论文中《The Google File System》\upcite{HDFS}的实现。整个HDFS 集群有Namenode 和Datanode构成master-worker（主从）模式。Namenode复杂构建命名空间，管理文件的元数据等，而Datanode负责实际存储数据，负责读写工作。HDFS整体框架图~\ref{fig:hdfsfw}所示：
\begin{figure}[htp]
\centering
\includegraphics{hdfsfw}
\caption{hdfs框架}
\label{fig:hdfsfw}
\end{figure}

HDFS在本文的设计中起分布式存储的作用，系统中的原始图像库，序列化图像库，特征库以及运行日志都保存在HDFS中。

\section{研究问题及研究现状}
本文研究的重点是如何加速海量图像库的特征提取工作，因为根据之前的分析，特征提取是基于图像内容检索的最为重要技术，并且耗时十分长，因此该研究问题实际意义十分强。

关于图像特征提取问题的研究，很早就有学者开展研究了，而特征提取发展中具有里程碑式的工作应当是Lowe等人在2000年提出的SIFT算法，该算法具有局部不变的特性，意味着在物体被旋转或者是被遮挡的情况下，都不会影响图像的识别，并且算法对光线、噪声、微视角改变的容忍度也相当高。基于这些特性，它们是高度显著而且相对容易撷取，在母数庞大的特征数据库中，很容易辨识物体而且鲜有误认，但是由于该算法的时间复杂性高，特征提取时间难以满足实时要求，后续有许多研究者在SIFT算法的基础上做改进，以提高特征提取的时间。

目前针对SIFT算法性能优化的研究主要集中在两个方面。一是改进算法以降低时间开销、提高特征提取速度。例如Bay提出的SURF（Speeded Up Robust Features）\upcite{SURF}延续SIFT算法的思想，通过积分图像和Haar小波相结合提高特征提取的速度；Ethan R.等人提出的ORB算法\upcite{ORB}采用了一种快速的基于Brief的二进制特征描述子方法提高了特征检测速度；Matas 等人则提出了最大极值区域（MSERS, Maximally stable Extremal Regions）特征检测方法\upcite{MSERS}，借用分水岭的思路检测图像中灰度最稳定的局域区域，然后对检测区域进行旋转和尺度的归一。二是借助FPGA、GPU等硬件加速器提高SIFT算法的性能，例如S.Heyman等人将SIFT算法移植到GPU上\upcite{GPUSIFT}，获得了不错的性能加速。本文则研究了基于Spark 优化SIFT 算法性能的方法。

在大数据和图像处理的交叉研究中，也有许多出色的研究工作，例如，Hadong Zhu 等和Akash K Sabarad 等分别在Hadoop 平台下实现了大规模图像特征提取\upcite{HadoopImage1,HadoopImage2}，相对于单机环境，获得了不错的加速比；Hanli Wang等设计了一个云平台下的大规模图像检索的处理框架CHCF\upcite{HadoopImage3}，Manimala Singha 等基于Hadoop 设计并实现了一个基于内容检索的图像检索框架\upcite{HadoopImage5}。 和上述工作不同的是，本文是基于Spark下进行图像处理技术的研究。

大数据处理框架Spark自在Apach上开源以来，一直都受到热烈的关注，有许多研究者都使用Spark进行数据的处理，包括现在十分火热的机器学习，深度学习都有在Spark上应用，比如基于Spark的机器学习库MLib\upcite{Mlib}，将Spark和deep learning 结合的进行移动数据分析和研究\upcite{SparkDeepLearing}，基于Spark的对规模图计算\upcite{SparkGraph}，但是我们发现Spark现在更多的是进行纯文本的分析，直接对图像进行处理的尚未发现，并且Spark 里面也没有对图像处理支持的相关库，因此这也促使我们产生在Spark 上进行图像处理的相关研究，因此我们就选取了图像处理中的特征提取步骤，在Spark 上开展研究。

\section{本文工作与成果}
\begin{figure}[htp]
\centering
\includegraphics{workfw}
\caption{工作成果展示}
\label{fig:workfw}
\end{figure}
如图~\ref{fig:workfw}所示，针对大规模图像特征提取，我们设计了一个基于Spark大规模图像特征提取系统Spark-SIFT，然后，我们针对该系统提出了三种优化方案。首先，我们针对单个图片体积较少，而HDFS读写小文件效率又不高问题，我们对系统进行了序列化优化。然后，针对Spark现有的任务划分方式造成的负载不均衡问题，我们提出了分割式的特征提取算法。最后，我们针对分割式算法的基础上，进一步优化，提出了Shuffle-efficient 的特征提取算法。具体研究内容概括如下：
\begin{compactenum}
\item 大规模图像库分布式提取系统Spark-SIFT\\本文首先实现了一个Spark下的图像处理基础库SparkImgLib，然后基于SparkImgLib，本文实现了Spark 框架下的SIFT算法。在此基础上设计了一个大规模的图像库特征提取系统。
\item key-value图片描述是数据结构\\spark在加载众多小文件时性能较低，而处理的图片体积一般是比较小的，比如几百K，因此本文设计中采用用key-value的方式描述一张图片，将图片以record 的形式序列化保存到HDFS 中，从而使得Spark可以一次性加载多条记录，提升了读写的性能。
\item 分割式特征提取算法\\Spark在进行任务划分时只考虑了task总体积，但是这种划分方式在本文设计中可能会造成tasks的负载不均衡，因为图片本身大小也会影响tasks的运行时间，针对这一点，我们提成了分割式的图片特征提取算法，将较大的图片分割成子块，来改善Spark在因为tasks划分方式导致的负载不均衡问题。
\item Shuffle-efficient特征提取算法\\Spark中Shuffle机制是影响性能的重要因素。分割式提取算法中存在Shuffle问题，比如同一图片的子块散落在不同的分区时的数据混洗以及在不同分区上收集同一张图片子块时的数据混洗。针对上述Shuffle问题，我们提出了shuffle-efficient的特征提取算法，提升shuffle性能。
\end{compactenum}

\section{论文结构}
本文一共分为6个章节，总体结构如图~\ref{fig:paperfw}所示：
\begin{figure}[htp]
\centering
\includegraphics{paperfw}
\caption{论文组织结构}
\label{fig:paperfw}
\end{figure}

第一章为绪论，在绪论中，我们首先讨论了基于文本的图像检索技术和基于内容的图像检索技术的区别，解释了基于内容的图像检索技术存在意义，进而引出了基于内容的图像检索中的两个关键技术，分别是图像特征提取技术和图像匹配技术。对于特征提取部分，我们阐述了图像特征的分类及对应的提取方法。对于特征匹配部分，我们解释了匹配的基本原理及几种度量方式的特点和区别。在介绍完图像关键处理技术后，我们分析了大数据处理技术，在这部分内容中，我们主要介绍了本系统设计中的两个大数据处理框架Spark和HDFS，我们分析了它们的存在的价值，技术特点，及在本文设计中扮演的角色。最后，介绍了本文的研究问题及问题研究现状，本文工作成果以及论文结构三部分。

第二章本文分析了设计中所涉及的一些原理知识，这些知识为本文的设计工作提供理论基础。

第三章介绍了大规模图像库特征提取系统Spark-SIFT系统的整体设计工作。

第四章介绍了在Spark-SIFT基础上提出的三种优化方案，分别是key-value的图片描述方式，分割式特征提取算法以及shuffle-efficient特征提取算法。

第五章进行实验数据的分析

第六章总结了本文的所有设计工作及创新点，分析了存在的不足点，最后对未来工作的一些看法和展望。


