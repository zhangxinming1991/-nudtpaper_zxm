\begin{cabstract}
当代社会互联网上图片的数据量急剧增长，而用户的检索需求越来越高，传统基于文本的图片检索技术因其描述词汇受限，很难在满足大数据背景下的图片检索。如何快速,准确的从众多图片中找到目标图片或者相似图片，成为了研究的热点。基于图像内容的检索技术在该背景下受到研究者们的关注，该技术使用图片自身的特征去匹配目标图片，检索结果具有非常高的准确性，百度和谷歌公司已经相继推出的以图搜图的服务，用户体验十分好。基于图片内容的检索技术包含了许多图像处理的技术，其中特征提取技术最为关键。

特征提取是基于内容的图像检索技术中的关键步骤，因为后续其他的图像处理步骤是在此基础上进行的。在众多的特征提取算法中，SIFT 算法最为著名，该算法提取的特征点与图像的旋转，大小无关，对于噪声，光线的容忍度也相当高，是一个划时代的特征提取算法。但是该算法的时间复杂度为指数级别，难以满足对处理时间要求特别高的的应用场景。因此后续有众多研究者对该算法进行优化研究，主要分为算法本身优化和通过硬件加速两方面。改进算法分别有SURF、ORB以及MSERS等。在硬件加速方式下，在GPU和FPGA 下均有SIFT 算法的实现。本文的工作和硬件加速是同一类型的研究工作，不同的地方在于本文是基于大数据处理框架Spark 进行加速的，研究的是大数据背景下的特征提取加速。在众多大数据处理框架中，Spark是一个内存计算类型的数据处理框架，在处理速度上有明显的优势。在此之前，暂时还没有研究者在Spark 上进行大规模图像库特征提取工作的研究，于是本文基于Spark 处理框架和SIFT 算法，在上面开展大规模图像库特征提取的研究工作。

在本文中，我们设计了一个基于spark 的大规模图像特征提取系统Spark-SIFT。该系统框架主要包含三部分：1）Spark图像基础库Spark-imageLib模块；2）Spark-sift特征提取功能模块；3）图片的序列化模块。之后本文又针对Spark-SIFT系统提出了三种优化方案。第一，因为图片的体积相对Spark来说普遍较小，而Spark在加载众多小文件时读写效率很低，针对这一现象，本文提出了Key-Vaule的图片描述方式，将图片转化成记录的形式，再将记录合并保存以提高Spark的加载效率。第二，Spark在进行任务划分时仅考虑任务的总体积，而忽略任务中图片尺度大小，这一任务划分机制导致Spark-SIFT 在处理图片大小相差较大的数据集时出现的负载不均衡问题，针对这一问题，本文提出了分割式特征提取算法，该算法核心思想是分而治之，先将大图片分割成小子块，并行处理，之后再统一收集，通过这种方式避免因为处理图片的尺度大小而导致负载不均衡现象。第三，本文针对分割式算法中引入的Shuffle操作，进一步提出了Shuffle-Efficient 分割式提取算法，通过高效的分区策略减少跨分区收集同一张图片子块的网络开销。 实验结果表明，本文提出并且设计的Spark-SIFT大规模图像特征提取框架取得了较好的加速效果。使用7 台机器，处理4G 图片集合，相对于单机提取，加速比达到了19.5，优于GPU 的加速比。 Key-Value 的图片描述方式在加载11G图片数据集时，加载性能相对binaryFile方式提升了61.7\%,相对于objectFile方式提升了83.3\%；分割式提取算法较不分割提取算法在处理480M 图片集合将提取速度进一步提高了7.8 倍；Shuffle-Efficient 分割式特征提取算法有效的减少在收集图片子块时的网络传输开销，在处理6.8G图片数据集时，高效的分区策略相对于Hash 分区策略，收集的性能提高了29.7\%。
\end{cabstract}
\ckeywords{大数据; 图像特征提取; Spark; SIFT算法; 负载均衡;}

\begin{eabstract}
Nowadays, the amount of pictures on the Internet has increased dramatically, and how to quickly and accurately find the target pictures or similar pictures from many pictures has been become a hot spot of research. The content-based image retrieval has becomes more popular which has more accurate and better in user experience. The content-based image retrieval contains a lot of technique of image processing, including feature extraction technology which is most important technique.

Feature extraction is a key step in pattern recognition and content based image retrieval, because other subsequent image processing steps are carried out on this basis.SIFT algorithm is the most famous among other feature extraction algorithms and the extracted feature points is not effected by the rotation and size of image. What is more, its tolerance for noise, the light is quite high, So SIFT is a landmark feature extraction algorithm. However, the time complexity of the algorithm is exponential, so it is difficult to meet the requirements of real-time. Therefore, many researchers have optimized the algorithm, which can be divided into two aspects: the optimization of the algorithm itself and the acceleration by the hardware. The improved algorithms are SURF, ORB and MSERS respectively. Hardware acceleration is divided into GPU and FPGA. The work in this paper is similar as hardware acceleration, but it based on Spark ,a big data processing framework. In many large data processing frameworks, spark is a memory based data processing framework, which has obvious advantages in processing speed.Prior to this, yet no research on large-scale image database features in Spark extraction, this paper Spark processing framework based on the research work carried out in a large image database in the above feature extraction.

In this paper, we design a large scale image feature extraction framework based on spark. The framework consists of three parts: 1) image basic processing interface; 2) SIFT feature extraction algorithm; 3) image serialization. Aiming at the problem that the picture size is too large in the picture set, which leads to unbalanced load, we propose a segmentation feature extraction method. The large image is divided into small blocks, so as to improve the parallelism.In view of the shuffle problem in the segmentation algorithm, we further propose the shuffle-effient split type extraction algorithm.

Experimental results show that our framework achieves good speedup. With 7 machines, the 4G image set is processed, and the speedup is 19.5 compared with the single machine extraction. The split type extraction algorithm can further improve the extraction speed by 7.8 times compared with the non segmentation algorithm in dealing with the 480M image set. Shuffle-effient segmentation feature extraction algorithm effectively reduces the shuffle size, and further improves the feature extraction time.

\end{eabstract}
\ekeywords{Bigdata; Image feature extraction; Spark; SIFT algorithm; Load balance}

