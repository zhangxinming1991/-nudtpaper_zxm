\chapter{总结和展望}
\section{全文总结}
随着大数据的不断发展，我们被越来越多的数字信息包围，如何快速的处理海量的数据逐渐成为研究的热点，比如基于图像内容的检索技术就是在该背景下发展成为研究热点，因为现在互联网上的图片越来越多，传统的基于文本的检索技术已不能应对在海量数据中准确检索的要求了。于是本文选取了图像内容检索技术中的最重要步骤特征提取技术作为研究点，研究海量图像库的快速准确特征提取技术。本文的工作可以归纳为以四点：
\begin{compactenum}
\item 设计了一个基于Spark的大规模特征提取系统Spark-SIFT。本文选取了特征提取中具有划时代意义的算法SIFT算法作为Spark下的特征提取算法，该算法精度高，并且具有局部不变性，在目标图片被遮挡或者旋转的情况下依然可以被识别，但是该算法时间复杂度高，实时性差，于是本文根据之前研究者对SIFT算法优化的思路，提出了使用Spark进行SIFT算法加速的设计思路。首先，本文在Spark上设计了一个图像处理的基础库Spark-imageLib，该库提供了图像的表示，图像的常用基础操作等，然后基于该Spark-imageLib 库，本文设计了Spark-sift算法，最后在Spark应用程序中调用Spark-sift算法，进行大规模的图像特征提取；
\item 针对Spark-SIFT系统的加载处理图片低效问题，本文提出了Key-Value的图片描述数据结构。因为现在互联网下图片的体积普遍较小，Spark在加载众多小图片时，会有频繁的磁盘IO 操作，加载的效率比较低。于是，本文提出了一个以key-value描述图片的数据结构，将单张图片转化为一条记录，记录的key为文件名，value为图片的内容字节流，然后将众多的记录序列化保存到HDFS中，之后Spark在加载时就可以一次性加载大量的记录，从而提高了Spark加载处理图片的效率；
\item 针对Spark-SIFT系统的负载不均衡问题，本文提出了分割式特征提取算法。Spark-SIFT在进行特征提取时，我们发现Spark在处理图片大小差异较大的数据集时，会出现负载不均衡的现象，这是由于Spark在任务划分时只考虑了任务的总体积，忽略了图片尺度对任务运行时间的影响。于是本文提出了分割式的特征提取算法，将图片划分为子块，在各个子块上进行特征提取，最后归并在一起。实验数据表明分割式特征提取算法即提高了处理的并行度也解决了负载均衡的问题；
\item 针对分割式算法中存在的Shuffle操作，提供一种Shuffle-Efficient的特征提取算法。因为分割式提取算法虽然提高了并行度，但是也将Shuffle操作引进来了，Shuffle操作会带来频繁的磁盘IO和网络传输开销，十分影响性能。因此，本文提出了高效的分区划分策略，对子块的key值的`\#`字符前的字符串求哈希值，再根据哈希值对分区总数的求余值得到分区号，通过这种方法，使得同一图片的子块散落在同一分区上，从而减少了跨分区收集子块的网络开销；
\end{compactenum}
\section{未来工作展望}
随着4K,8K技术的发展，往后的图片的体积会越来越大，传统的技术技术肯定会面临着许多的问题，我们会对Spark下4K,8K图像处理问题展开研究。同时，我们也会在本文的基础上进行图像内容检索的研究，尝试将大规模的图像库的提取和匹配工作结合一起。最后，因为视频是由一帧帧图片组成的，因此Spark下如何高效进行视频处理也是一个值得深入研究的问题。
